{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISMIR 2018 Tutorial\n",
    "# Deep Learning for Music Information Retrieval\n",
    "\n",
    "## Part 1: Convoluational Neural Networks for Instrumental, Genre and Mood Detection in Music\n",
    "\n",
    "Author: Thomas Lidy\n",
    "\n",
    "This tutorial shows how different Convolutional Neural Network architectures are used for:\n",
    "* Instrumental vs. Vocal Detection:  detecting whether a piece of music is instrumental or contains vocals\n",
    "* Genre Classification\n",
    "* Mood Recognition\n",
    "\n",
    "The data set used is the [MagnaTagATune Dataset](http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset), but a smaller subset of it, with only 1 sample excerpt of each of the original audio files.\n",
    "\n",
    "It consists of 5405 files, each 30 seconds long. \n",
    "\n",
    "The annotations for this dataset contain a multitude of tags, including some that hint at whether the file is instrumental or vocal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "This tutorial contains:\n",
    "* Loading and Preprocessing of Audio files\n",
    "* Loading class files from CSV and using Label Encoder\n",
    "* Audio Preprocessing: Generating log Mel spectrograms\n",
    "* Standardization of Data\n",
    "* Convolutional Neural Networks\n",
    "* Train/Test set split\n",
    "\n",
    "* Instrumental vs. Vocal Detection\n",
    "* Genre Classification\n",
    "* Mood Recognition\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* Python >= 3.5\n",
    "* Keras >= 2.1.1\n",
    "* Tensorflow\n",
    "* scikit-learn >= 0.18\n",
    "* Pandas\n",
    "* Librosa\n",
    "* MatplotLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "If you haven't already (follwing [README](../README.md#download-prepared-datasets)), \n",
    "please download the following data sets for this tutorial:\n",
    "\n",
    "#### MagnaTagAtune\n",
    "\n",
    "Prepared Spectrograms: https://owncloud.tuwien.ac.at/index.php/s/VyDlQKmsA2EFAhv (209MB)\n",
    "\n",
    "Unzip them into a folder, e.g. inside this Tutorial folder, and adapt the following `DATA_PATH` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PATH OF DOWNLOADED DATA HERE\n",
    "# '.' is ok if you unzipped the files inside this tutorials folder\n",
    "\n",
    "METADATA_PATH = 'metadata'\n",
    "DATA_PATH = './ISMIR2018_tut_prepared_features'\n",
    "\n",
    "from os.path import join\n",
    "SPECTROGRAM_FILE = join(DATA_PATH, 'ISMIR2018_tut_Magnagtagatune_spectrograms.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU USE A GPU, you may set which GPU(s) to use here:\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\" #\"0,1,2,3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd # Pandas for reading CSV files and easier Data handling in preparation\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "\n",
    "# Machine Learning preprocessing and evaluation\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports for audio reading and processing\n",
    "#import audio_spectrogram as rp\n",
    "#from audiofile_read import audiofile_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Metadata\n",
    "\n",
    "The tab-separated file contains pairs of filename TAB class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3023, 189)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = os.path.join(METADATA_PATH,'annotations_final_subsample.csv')\n",
    "\n",
    "# we select the last column (-1) as the index column (= filename)\n",
    "metadata = pd.read_csv(csv_file, index_col=0, sep='\\t')\n",
    "\n",
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_id</th>\n",
       "      <th>no voice</th>\n",
       "      <th>singer</th>\n",
       "      <th>duet</th>\n",
       "      <th>plucking</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>world</th>\n",
       "      <th>bongos</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>female singing</th>\n",
       "      <th>...</th>\n",
       "      <th>female singer</th>\n",
       "      <th>rap</th>\n",
       "      <th>metal</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>quick</th>\n",
       "      <th>water</th>\n",
       "      <th>baroque</th>\n",
       "      <th>women</th>\n",
       "      <th>fiddle</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp3_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3</th>\n",
       "      <td>1119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3</th>\n",
       "      <td>6021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3</th>\n",
       "      <td>11847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3</th>\n",
       "      <td>17119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3</th>\n",
       "      <td>25118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    clip_id  no voice  singer  \\\n",
       "mp3_path                                                                        \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     1119         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     6021         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    11847         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    17119         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    25118         0       0   \n",
       "\n",
       "                                                    duet  plucking  hard rock  \\\n",
       "mp3_path                                                                        \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "\n",
       "                                                    world  bongos  \\\n",
       "mp3_path                                                            \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "\n",
       "                                                    harpsichord  \\\n",
       "mp3_path                                                          \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "\n",
       "                                                    female singing   ...     \\\n",
       "mp3_path                                                             ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "\n",
       "                                                    female singer  rap  metal  \\\n",
       "mp3_path                                                                        \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "\n",
       "                                                    hip hop  quick  water  \\\n",
       "mp3_path                                                                    \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "\n",
       "                                                    baroque  women  fiddle  \\\n",
       "mp3_path                                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "\n",
       "                                                    english  \n",
       "mp3_path                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "\n",
       "[5 rows x 189 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no voice</th>\n",
       "      <th>singer</th>\n",
       "      <th>duet</th>\n",
       "      <th>plucking</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>world</th>\n",
       "      <th>bongos</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>female singing</th>\n",
       "      <th>clasical</th>\n",
       "      <th>...</th>\n",
       "      <th>female singer</th>\n",
       "      <th>rap</th>\n",
       "      <th>metal</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>quick</th>\n",
       "      <th>water</th>\n",
       "      <th>baroque</th>\n",
       "      <th>women</th>\n",
       "      <th>fiddle</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clip_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         no voice  singer  duet  plucking  hard rock  world  bongos  \\\n",
       "clip_id                                                               \n",
       "1119            0       0     0         0          0      0       0   \n",
       "6021            0       0     0         0          0      0       0   \n",
       "11847           0       0     0         0          0      0       0   \n",
       "17119           0       0     0         0          0      0       0   \n",
       "25118           0       0     0         0          0      0       0   \n",
       "\n",
       "         harpsichord  female singing  clasical   ...     female singer  rap  \\\n",
       "clip_id                                          ...                          \n",
       "1119               0               0         0   ...                 0    0   \n",
       "6021               0               0         0   ...                 0    0   \n",
       "11847              0               0         0   ...                 0    0   \n",
       "17119              0               0         0   ...                 0    0   \n",
       "25118              0               0         0   ...                 0    0   \n",
       "\n",
       "         metal  hip hop  quick  water  baroque  women  fiddle  english  \n",
       "clip_id                                                                 \n",
       "1119         0        0      0      0        0      0       0        0  \n",
       "6021         0        0      0      0        0      0       0        0  \n",
       "11847        0        0      0      0        0      0       0        0  \n",
       "17119        0        0      0      0        0      0       0        0  \n",
       "25118        0        0      0      0        0      0       0        0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make the clip_id the index and drop the orignal mp3_path\n",
    "# as we later align by clip_id\n",
    "\n",
    "metadata.set_index('clip_id', inplace=True)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Instrumental vs. Vocal Detection\n",
    "\n",
    "this is a binary classification task (output decision is between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2 classes from a list of tags\n",
    "\n",
    "There are plenty of \"tags\" in this data set which hint at wether a track is \"vocal\" or \"instrumental\". We group these tags and finally come up with 1 boolean column saying whether a track is \"vocal\" or \"instrumental\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vocal = ['singer', 'female singing', 'female opera', 'male vocal', 'vocals', 'men', 'female', 'female voice', 'voice', 'male voice', 'girl', 'chanting', 'talking', 'choral', 'male singer', 'man singing', 'male opera', 'chant', 'man', 'female vocal', 'male vocals', 'vocal', 'woman', 'woman singing', 'singing', 'female vocals', 'voices', 'choir', 'female singer', 'women', 'choir', 'women']\n",
    "\n",
    "tags_instrumental = ['instrumental', 'no voice', 'no voices', 'no vocals', 'no vocal', 'no singing', 'no singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[tags_vocal].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vocal to True of any of the tags_vocal are 1\n",
    "gt_vocal = metadata[tags_vocal].any(axis=1)\n",
    "gt_vocal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set instrumental to True of any of the tags_instrumental are 1\n",
    "gt_instrumental = metadata[tags_instrumental].any(axis=1)\n",
    "gt_instrumental.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We can only use the tag if EITHER instrumental OR vocal is True.</b><br>\n",
    "If both of them are True or both of them are False, we cannot trust the groundtruth data. Ergo we have to remove these and retain only the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain = np.logical_xor(gt_vocal,gt_instrumental)\n",
    "retain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_orig = len(gt_vocal)\n",
    "n_retain = sum(retain)\n",
    "\n",
    "print(\"For instrumental vs. vocal, from originally\", n_orig, \"input examples, we can only retain\",n_retain, \"trusted ones in our groundtruth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we cut from gt_vocal only the instances to retain. If they are True they are vocal, if they are False, they are instrumental:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_final = gt_vocal[retain]\n",
    "gt_final.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(sum(gt_final)) + \" vocal tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(sum(np.logical_not(gt_final))) + \" instrumental tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create two lists: one with the clip_ids and one with associated classes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index in list of clip_ids\n",
    "clip_ids = gt_final.index.tolist()\n",
    "# convert boolean to int and store in other list\n",
    "classes = (gt_final * 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_ids[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Numpy array as needed by Keras\n",
    "classes = np.array(classes)\n",
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Spectrograms\n",
    "\n",
    "We have pre-processed the audio files already and extracted Mel spectrograms. We load these from a Numpy .npz file, which contains the spectrograms and also the associated clip ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6380, 80, 80)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with np.load(SPECTROGRAM_FILE) as npz:\n",
    "    spectrograms = npz[\"features\"]\n",
    "    spec_clip_ids = npz[\"clip_id\"]\n",
    "    \n",
    "spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6380"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spec_clip_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     spec_id\n",
       "66         0\n",
       "90         1\n",
       "91         2\n",
       "94         3\n",
       "105        4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe that associates the order of the spectrograms with the clip_ids\n",
    "spectrograms_clip_ids = pd.DataFrame({\"spec_id\": np.arange(spectrograms.shape[0])}, index = spec_clip_ids)\n",
    "spectrograms_clip_ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Mel Spectrogram (1 example just for illustration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can skip this if you do not have matplotlib installed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first spectrogram as an example\n",
    "spec = spectrograms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot it \n",
    "fig = plt.imshow(spec, origin='lower', aspect='auto')\n",
    "fig.set_cmap('jet')\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "We use <b>Zero-mean Unit-variance standardization</b> (also known as Z-score normalization).\n",
    "Here, we use <b>attribute-wise standardization</b>, i.e. each pixel is standardized individually, as opposed to computing a single mean and single standard deviation of all values.\n",
    "\n",
    "('Flat' standardization would also be possible, but we have seen benefits of attribut-wise standardization in our experiments).\n",
    "\n",
    "We use the StandardScaler from the scikit-learn package for our purpose.\n",
    "As it works typically on vector data, we have to vectorize (i.e. reshape) our matrices first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    # vectorize before standardization (cause scaler can't do it in that format)\n",
    "    N, ydim, xdim = data.shape\n",
    "    data = data.reshape(N, xdim*ydim)\n",
    "\n",
    "    # standardize\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    # reshape to original shape\n",
    "    return data.reshape(N, ydim, xdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = standardize(spectrograms)\n",
    "spectrograms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Metadata and Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Resolve numeric ids in *labels.csv files to clip ids from Magnatagatune set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2617, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_genre = pd.read_csv(join(METADATA_PATH, \"ismir2018_tut_part_2_genre_labels.csv\"), index_col=0)\n",
    "metadata_genre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2617, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_genre_clip_ids = pd.read_csv(join(METADATA_PATH, \"ismir2018_tut_part_2_genre_metadata.csv\"), index_col=0)\n",
    "metadata_genre_clip_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_id</th>\n",
       "      <th>mp3_path</th>\n",
       "      <th>track_number</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>url</th>\n",
       "      <th>segmentStart</th>\n",
       "      <th>segmentEnd</th>\n",
       "      <th>original_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15066</th>\n",
       "      <td>32957</td>\n",
       "      <td>D:/Research/Data/MIR/MagnaTagATune/mp3_full/7/...</td>\n",
       "      <td>7</td>\n",
       "      <td>Reckless Woman Blues</td>\n",
       "      <td>Jag</td>\n",
       "      <td>Cypress Grove Blues</td>\n",
       "      <td>http://www.magnatune.com/artists/albums/jag-cy...</td>\n",
       "      <td>117</td>\n",
       "      <td>146</td>\n",
       "      <td>http://he3.magnatune.com/all/07-Reckless%20Wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5256</th>\n",
       "      <td>11455</td>\n",
       "      <td>D:/Research/Data/MIR/MagnaTagATune/mp3_full/3/...</td>\n",
       "      <td>3</td>\n",
       "      <td>All Worn Out</td>\n",
       "      <td>Jag</td>\n",
       "      <td>Four Strings</td>\n",
       "      <td>http://www.magnatune.com/artists/albums/jag-four/</td>\n",
       "      <td>30</td>\n",
       "      <td>59</td>\n",
       "      <td>http://he3.magnatune.com/all/03-All%20Worn%20O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23603</th>\n",
       "      <td>52328</td>\n",
       "      <td>D:/Research/Data/MIR/MagnaTagATune/mp3_full/3/...</td>\n",
       "      <td>14</td>\n",
       "      <td>You're Cheatin' On Me</td>\n",
       "      <td>Jag</td>\n",
       "      <td>Four Strings</td>\n",
       "      <td>http://www.magnatune.com/artists/albums/jag-four/</td>\n",
       "      <td>88</td>\n",
       "      <td>117</td>\n",
       "      <td>http://he3.magnatune.com/all/14-You're%20Cheat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24083</th>\n",
       "      <td>53525</td>\n",
       "      <td>D:/Research/Data/MIR/MagnaTagATune/mp3_full/7/...</td>\n",
       "      <td>16</td>\n",
       "      <td>Baby Don't Say That</td>\n",
       "      <td>Jag</td>\n",
       "      <td>Cypress Grove Blues</td>\n",
       "      <td>http://www.magnatune.com/artists/albums/jag-cy...</td>\n",
       "      <td>88</td>\n",
       "      <td>117</td>\n",
       "      <td>http://he3.magnatune.com/all/16-Baby%20Don't%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20833</th>\n",
       "      <td>45825</td>\n",
       "      <td>D:/Research/Data/MIR/MagnaTagATune/mp3_full/4/...</td>\n",
       "      <td>11</td>\n",
       "      <td>How the story goes</td>\n",
       "      <td>Burnshee Thornside</td>\n",
       "      <td>The Art of Not Blending In</td>\n",
       "      <td>http://www.magnatune.com/artists/albums/burnsh...</td>\n",
       "      <td>146</td>\n",
       "      <td>175</td>\n",
       "      <td>http://he3.magnatune.com/all/11-How%20the%20st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       clip_id                                           mp3_path  \\\n",
       "15066    32957  D:/Research/Data/MIR/MagnaTagATune/mp3_full/7/...   \n",
       "5256     11455  D:/Research/Data/MIR/MagnaTagATune/mp3_full/3/...   \n",
       "23603    52328  D:/Research/Data/MIR/MagnaTagATune/mp3_full/3/...   \n",
       "24083    53525  D:/Research/Data/MIR/MagnaTagATune/mp3_full/7/...   \n",
       "20833    45825  D:/Research/Data/MIR/MagnaTagATune/mp3_full/4/...   \n",
       "\n",
       "       track_number                  title              artist  \\\n",
       "15066             7   Reckless Woman Blues                 Jag   \n",
       "5256              3           All Worn Out                 Jag   \n",
       "23603            14  You're Cheatin' On Me                 Jag   \n",
       "24083            16    Baby Don't Say That                 Jag   \n",
       "20833            11     How the story goes  Burnshee Thornside   \n",
       "\n",
       "                            album  \\\n",
       "15066         Cypress Grove Blues   \n",
       "5256                 Four Strings   \n",
       "23603                Four Strings   \n",
       "24083         Cypress Grove Blues   \n",
       "20833  The Art of Not Blending In   \n",
       "\n",
       "                                                     url  segmentStart  \\\n",
       "15066  http://www.magnatune.com/artists/albums/jag-cy...           117   \n",
       "5256   http://www.magnatune.com/artists/albums/jag-four/            30   \n",
       "23603  http://www.magnatune.com/artists/albums/jag-four/            88   \n",
       "24083  http://www.magnatune.com/artists/albums/jag-cy...            88   \n",
       "20833  http://www.magnatune.com/artists/albums/burnsh...           146   \n",
       "\n",
       "       segmentEnd                                       original_url  \n",
       "15066         146  http://he3.magnatune.com/all/07-Reckless%20Wom...  \n",
       "5256           59  http://he3.magnatune.com/all/03-All%20Worn%20O...  \n",
       "23603         117  http://he3.magnatune.com/all/14-You're%20Cheat...  \n",
       "24083         117  http://he3.magnatune.com/all/16-Baby%20Don't%2...  \n",
       "20833         175  http://he3.magnatune.com/all/11-How%20the%20st...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_genre_clip_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blues</th>\n",
       "      <th>classical</th>\n",
       "      <th>country</th>\n",
       "      <th>jazz</th>\n",
       "      <th>pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>techno</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clip_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32957</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11455</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52328</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53525</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45825</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         blues  classical  country  jazz  pop  rock  techno\n",
       "clip_id                                                    \n",
       "32957        1          0        0     0    0     0       0\n",
       "11455        1          0        0     0    0     0       0\n",
       "52328        1          0        0     0    0     0       0\n",
       "53525        1          0        0     0    0     0       0\n",
       "45825        1          0        0     0    0     0       0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the numeric id from metadata_genre by clip_id from metadata_genre_clip_ids\n",
    "\n",
    "clip_ids_sorted_by_index = metadata_genre_clip_ids.loc[metadata_genre.index]['clip_id']\n",
    "metadata_genre.index = clip_ids_sorted_by_index\n",
    "metadata_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2617"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metadata_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2617"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if we found all metadata clip ids in our spectrogram data\n",
    "len(set(metadata_genre.index).intersection(set(spec_clip_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes needs to be a \"1-hot encoded\" numpy array (which our groundtruth already is! we just convert pandas to numpy)\n",
    "classes = metadata_genre.values\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = metadata_genre.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) now from the clip_id ordered like the metadata get the correct spectrogram indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_ids_genre = metadata_genre.index\n",
    "spec_indices = spectrograms_clip_ids.loc[clip_ids_genre]['spec_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6380, 80, 80)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrograms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we select a subset of the original data for the metadata of each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2617, 80, 80)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = spectrograms[spec_indices,:]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --> our X and Y used in the following training procedures are called 'data' and 'classes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images or spectrograms, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which aggregates neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "### Adding the channel\n",
    "\n",
    "As CNNs were initially made for image data, we need to add a dimension for the color channel to the data. RGB images typically have a 3rd dimension with the color. \n",
    "\n",
    "<b>Spectrograms, however, are considered like greyscale images, as in the previous tutorial.\n",
    "Likewise we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_channel(data, n_channels=1):\n",
    "    # n_channels: 1 for grey-scale, 3 for RGB, but usually already present in the data\n",
    "    \n",
    "    N, ydim, xdim = data.shape\n",
    "\n",
    "    if keras.backend.image_data_format() == 'channels_last':  # TENSORFLOW\n",
    "        # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "        data = data.reshape(N, ydim, xdim, n_channels)\n",
    "    else: # THEANO\n",
    "        # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "        data = data.reshape(N, n_channels, ydim, xdim)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2617, 80, 80, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = add_channel(data, n_channels=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of files)\n",
    "input_shape = data.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Set Split\n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%).\n",
    "\n",
    "Note: \n",
    "For demo purposes we use only 1 split here. A better way to do it is to use **Cross-Validation**, doing the split multiple times, iterating training and testing over the splits and averaging the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INDEX: [ 521 2443 1071 ...  473  205  182]\n",
      "TEST INDEX: [2103 2215 1504  425  490  785  392 1534   27  765 1086 2318  156 2297\n",
      " 2422 2606 2359 2080 1732  744 1825 1545  402  966  390 2109 1541 1410\n",
      "   85  495  493  405  651   93  826 1608  906  463  523 1023  522 1163\n",
      "  594 2096 1171  347  333 2476  566 2373 1687 1250 1500 1926 1103 1228\n",
      " 2008 1801 1464 1281  991 1431 2111  650 1003 2205  489 2077 1179 1643\n",
      "  987  776 1513 1092   78 2545 1699  502 1373 1782 1863 2268   28 2394\n",
      " 2513 1563 1107 2286 1836 2142  873  345  884 1129 2159  141  224 2589\n",
      " 1299 2236 1157 2036 1907  928 1805  259 1690 2280 1932 1000  756 2575\n",
      " 1166  323  552  216 2515 2324 2407  973  528 2208   62 1639   20 2409\n",
      " 2314 2566   32 1535 1542  983    9   41 2605 1772  728 2238 1363 2222\n",
      "  250 2037  236  185 1517 2438  945 1788  851   68  253 2584  739 2540\n",
      " 1795 2102 2038 2567  315  876  903 1453 1551 2601  221 2046 1201  127\n",
      " 1420 2047 1333 2505  458  261 2149 1726 2603 1953  938 1791 1349  227\n",
      " 1274 1384  916 1570  607 1139 2579  948 1721  380 1543 2319 1110  467\n",
      "  641 1256 1729 2408 1089 2071  342 1717  624  545 2440  655 2361   33\n",
      " 2565  532  539  914 1291 1475 1707  251 2097  514  121 2201 1755 2040\n",
      " 2431 2282  742 1063 2126 2379   56 1435 1042 2216  422  904  474  719\n",
      "  321 2452  990  564 2561    7 1156  175 2421  706 1374 2204  654 1655\n",
      " 2189 1135 1587  856 1778  949  558   75 1823  965 1189 2490  598 1719\n",
      " 2070 1241 2245  573 1539 1347 1022  348 2473  748 1219  358  878 1184\n",
      " 1297 1713  374 1850 1341 1965  743    3  732 1097 1632 1920 2044  941\n",
      " 1745 1359  592  646 1552 1439 2248  223 1036 1824 1426  639  375  504\n",
      " 1427 1318 1843 2089  697  797 1505 1038  142 2148 1783 1270 1740 1649\n",
      "  891  868  228  335  712 2556 1709 2441  882 2466 2344 2300 1148  526\n",
      " 1765 2537  561 1246 1417  201 1340  449   98 1591 1405 1386 2244  993\n",
      "  444  180 1452 1930 1154 1353 1621 1615 1781 1502  927 2309 2264  230\n",
      " 1749 1301 1390   11 2260  255 1901 1083   97 1855  572 1600  615 2458\n",
      "  513  155  705  164  672 2368 1162 2602  478 1440 2402  613  130 2031\n",
      "  892  684 2294 2382  888  596 2573 1683  867  531   29 1128  208  487\n",
      " 2021 2405 1492 1796  313 2186 1835  404 1449 2312  937 2295 1899 2199\n",
      "  431  915 2553 1802 1355 2384  187 1271 1742  648 1668 2521 2164 2010\n",
      " 1133 1351 1482 2586 1404 2156 1496 1425  343 1556 1846 1982 1951  241\n",
      "  671 2145 1811 2360 2265 1971 1251 1581 2170 2056  147  219 1141  881\n",
      "  297  714  640 1273 1737 1509 1285 2234 1055 1489 1590 1691 1061  290\n",
      "  824 1956 1348  233  294  120 1571  453 1488 2310 2548 1974  889  611\n",
      "  821 1247 2155  694 1238   35   70 1841  109 2187 2211  652  464 1043\n",
      "  636 1978  716   79 2095 2117 1919  103  152 2165 1977 2485 2451  461\n",
      " 1807  102 1710 2455  667   92 2486  997 1939 2563 2320 1706 2174  303\n",
      " 1611 1158 1991   46 1127  777  207  784 2291 2014 1526   86  846 2487\n",
      " 2212  229  161 1397  331 1678 2232 2092  471  451 2475  163  346  231\n",
      "  802  955   48 1786  702 1964 1582  647 1381 1961 1312 1970 1560 1026\n",
      " 2450 1065 1005 1232  536 1455  173  542  701 1944 1862 2328 2045 1790\n",
      " 1585  838 1731 1100 2374 1474  972  626 1598 2390  276 1111 1711  280\n",
      " 1960 1037 2058 1178 1992 1514 2454  823 1546 2583 1068 1604 2581 2018\n",
      " 1885 2493   59 1562  860  316 1620  530  352 1183 2011 1938 1575  844\n",
      "  153 1828 1290 1838  976 1559  749  618 1479 1822 1814  472 1837  971\n",
      " 1527 1112  783  105 2501 1623 2506 2302 2585  477 2396]\n"
     ]
    }
   ],
   "source": [
    "# Stratified Split retains the class balance in both sets\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes[train_index]\n",
    "    test_classes = classes[test_index]\n",
    "# Note: this for loop is only executed once if n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1962, 80, 80, 1)\n",
      "(655, 80, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: Class 0: [1940 1587 1763 1682 1626 1587 1587] Class 1: [ 22 375 199 280 336 375 375]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(\"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network Models in Keras\n",
    "\n",
    "## Sequential Models\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are simple concatenations of layers. Graph models can also handle those but also more complex neural network architectures. Keras now recommends to use the Graph models by default, but for a simple entry into the topic we are going to start with Sequential models first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try different configurations,(comment/uncomment code blockes in the following code box:\n",
    "* 1 Layer\n",
    "* 2 Layer\n",
    "* change number of conv_filters\n",
    "* Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(0) # make results repeatable\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, (3, 3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, (3, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256, activation='sigmoid')) \n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "#model.add(Dense(1,activation='sigmoid'))\n",
    "model.add(Dense(n_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 78, 78, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 39, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 39, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               2654464   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 2,665,831\n",
      "Trainable params: 2,665,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1962/1962 [==============================] - 12s 6ms/step - loss: 0.3876 - acc: 0.8573\n",
      "Epoch 2/15\n",
      "1962/1962 [==============================] - 12s 6ms/step - loss: 0.3510 - acc: 0.8603\n",
      "Epoch 3/15\n",
      "1962/1962 [==============================] - 13s 7ms/step - loss: 0.3402 - acc: 0.8682\n",
      "Epoch 4/15\n",
      "1962/1962 [==============================] - 12s 6ms/step - loss: 0.3326 - acc: 0.8712\n",
      "Epoch 5/15\n",
      "1962/1962 [==============================] - 13s 7ms/step - loss: 0.3273 - acc: 0.8735\n",
      "Epoch 6/15\n",
      "1962/1962 [==============================] - 12s 6ms/step - loss: 0.3222 - acc: 0.8759\n",
      "Epoch 7/15\n",
      "1962/1962 [==============================] - 13s 6ms/step - loss: 0.3183 - acc: 0.8759\n",
      "Epoch 8/15\n",
      "1962/1962 [==============================] - 14s 7ms/step - loss: 0.3147 - acc: 0.8769\n",
      "Epoch 9/15\n",
      "1962/1962 [==============================] - 13s 7ms/step - loss: 0.3104 - acc: 0.8777\n",
      "Epoch 10/15\n",
      "1962/1962 [==============================] - 12s 6ms/step - loss: 0.3070 - acc: 0.8783\n",
      "Epoch 11/15\n",
      "1962/1962 [==============================] - 15s 7ms/step - loss: 0.3033 - acc: 0.8797\n",
      "Epoch 12/15\n",
      "1962/1962 [==============================] - 14s 7ms/step - loss: 0.3001 - acc: 0.8803\n",
      "Epoch 13/15\n",
      " 640/1962 [========>.....................] - ETA: 11s - loss: 0.3077 - acc: 0.8768"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always execute this, and then a box of accuracy_score below to print the result\n",
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layers\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layers + 32 convolution filters\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer + 32 convolution filters + Dropout\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parameters & Techniques\n",
    "\n",
    "Try out more parameters and techniques: (comment/uncomment code blocks below)\n",
    "* Adding ReLU activation\n",
    "* Adding Batch normalization\n",
    "* Adding Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "filter_size = (3,3)\n",
    "pool_size = (2,2)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, filter_size, padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=pool_size)) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, filter_size, padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=pool_size)) \n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256))  \n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIREX 2015 model\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 15   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, (12, 8), padding='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(Activation('sigmoid')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 1))) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid'))  \n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and training the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying Accuracy on Test Set\n",
    "\n",
    "test_pred = model.predict_classes(test_set)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO? Parallel CNNs\n",
    "\n",
    "It has been discovered, that CNNs for music work best, when they have one filter that is detecting frequencies in the vertical axis, and nother filter that is focused on the time axis, i.e. detecting rhythm. Consequently, this is realized in a parallel CNN, where 2 layers are not stacked after each other, but first run independently in parallel with their output being merged later.\n",
    "\n",
    "To create parallel CNNs we need a \"graph-based\" model. In Keras 1.x this is realized via the functional API of the Model() class.\n",
    "We use it to create two CNN layers that run in parallel to each other and are merged subsequently.\n",
    "In the functional API, you pass the name of the previous layer in (brackets) after defining the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO import from Part1a Music _speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact CNN\n",
    "\n",
    "This is a 5 layer Convolutional Neural Network inspired and adapted from Keunwoo Choi (https://github.com/keunwoochoi/music-auto_tagging-keras)\n",
    "\n",
    "It is specified using Keras' functional Model Graph API (https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactCNN(input_shape, nb_conv, nb_filters, n_mels, normalize, nb_hidden, dense_units, \n",
    "               output_shape, activation, dropout, multiple_segments=False, graph_model=False, input_tensor=None):\n",
    "    \n",
    "    melgram_input = Input(shape=input_shape)\n",
    "\n",
    "    if n_mels >= 256:\n",
    "        poolings = [(2, 4), (4, 4), (4, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 128:\n",
    "        poolings = [(2, 4), (4, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 96:\n",
    "        poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 72:\n",
    "        poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (3, 4)]\n",
    "    elif n_mels >= 64:\n",
    "        poolings = [(2, 4), (2, 4), (2, 5), (2, 4), (4, 4)]\n",
    "\n",
    "    # Determine input axis\n",
    "    if keras.backend.image_dim_ordering() == 'th':\n",
    "        channel_axis = 1\n",
    "        freq_axis = 2\n",
    "        time_axis = 3\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "        freq_axis = 1\n",
    "        time_axis = 2\n",
    "            \n",
    "    # Input block\n",
    "    #x = BatchNormalization(axis=time_axis, name='bn_0_freq')(melgram_input)\n",
    "        \n",
    "    if normalize == 'batch':\n",
    "        x = BatchNormalization(axis=freq_axis, name='bn_0_freq')(melgram_input)\n",
    "    elif normalize in ('data_sample', 'time', 'freq', 'channel'):\n",
    "        x = Normalization2D(normalize, name='nomalization')(melgram_input)\n",
    "    elif normalize in ('no', 'False'):\n",
    "        x = melgram_input\n",
    "\n",
    "    # Conv block 1\n",
    "    x = Convolution2D(nb_filters[0], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn1')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[0], name='pool1')(x)\n",
    "        \n",
    "    # Conv block 2\n",
    "    x = Convolution2D(nb_filters[1], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn2')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[1], name='pool2')(x)\n",
    "        \n",
    "    # Conv block 3\n",
    "    x = Convolution2D(nb_filters[2], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn3')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[2], name='pool3')(x)\n",
    "    \n",
    "    # Conv block 4\n",
    "    if nb_conv > 3:        \n",
    "        x = Convolution2D(nb_filters[3], (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='bn4')(x)\n",
    "        x = ELU()(x)   \n",
    "        x = MaxPooling2D(pool_size=poolings[3], name='pool4')(x)\n",
    "        \n",
    "    # Conv block 5\n",
    "    if nb_conv == 5:\n",
    "        x = Convolution2D(nb_filters[4], (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='bn5')(x)\n",
    "        x = ELU()(x)\n",
    "        x = MaxPooling2D(pool_size=poolings[4], name='pool5')(x)\n",
    "\n",
    "    # Flatten the outout of the last Conv Layer\n",
    "    x = Flatten()(x)\n",
    "      \n",
    "    if nb_hidden == 1:\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units, activation='relu')(x)\n",
    "    elif nb_hidden == 2:\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units[0], activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units[1], activation='relu')(x) \n",
    "    else:\n",
    "        raise ValueError(\"More than 2 hidden units not supported at the moment.\")\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(output_shape, activation=activation, name = 'output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(melgram_input, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Convolutional Layers\n",
    "nb_conv_layers = 4\n",
    "\n",
    "# number of Filters in each layer\n",
    "nb_filters = [64,64,64,128,128]\n",
    "\n",
    "# number of hidden layers at the end of the model\n",
    "nb_hidden = 1 # 2\n",
    "\n",
    "# how many neurons in each hidden layer\n",
    "dense_units = 128 #[128,56]\n",
    "\n",
    "# how many output units\n",
    "# IN A BINARY CLASSIFICATION TASK with 2 possible outputs, 1 single output unit is sufficent (deciding between 0 and 1)\n",
    "output_shape = 1\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A BINARY CLASSIFICATION TASK sigmoid activation is the right choice (activating betwee 0 and 1)\n",
    "output_activation = 'sigmoid'\n",
    "\n",
    "# which type of normalization\n",
    "normalization = 'batch'\n",
    "\n",
    "# droupout\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CompactCNN(input_shape, nb_conv = nb_conv_layers, nb_filters= nb_filters, n_mels = 96, \n",
    "                           normalize=normalization, \n",
    "                           nb_hidden = nb_hidden, dense_units = dense_units, \n",
    "                           output_shape = output_shape, activation = output_activation, \n",
    "                           dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "# the loss for a binary classification task is BINARY crossentropy\n",
    "loss = 'binary_crossentropy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "# simple case:\n",
    "# Stochastic Gradient Descent\n",
    "#optimizer = 'sgd' \n",
    "\n",
    "# advanced:\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.01)#lr=0.001 decay = 0.03\n",
    "adagrad = optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# We use mostly ADAM\n",
    "adam = optimizers.Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.01)\n",
    "nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, schedule_decay=0.004)\n",
    "\n",
    "# choose\n",
    "optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "metrics = ['accuracy', precision, recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other\n",
    "batch_size = 32 \n",
    "\n",
    "epochs = 30\n",
    "\n",
    "validation_split=0.1 \n",
    "\n",
    "#n_folds = 5\n",
    "random_seed = 0\n",
    "\n",
    "callbacks = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#home_dir = os.getenv(\"HOME\")\n",
    "\n",
    "#TB_LOGDIR = os.path.join(home_dir, \"./tensorboard\")\n",
    "\n",
    "TB_LOGDIR = \"./tensorboard\"\n",
    "\n",
    "experiment_name = \"instrumental\"\n",
    "\n",
    "tb_logdir_cur = os.path.join(TB_LOGDIR, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# new tensorboard callback at each training\n",
    "# tensorboard_run_id = \"Vocal_magna_2seg_adam_compact_128fbis_128h\"\n",
    "# tb_logdir = \"%s/%s_fold%d %s\" %(tb_logdir, tensorboard_run_id, fold, strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execute the following in a terminal:\\n\")\n",
    "print(\"tensorboard --logdir=\" + TB_LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TensorBoard in Python\n",
    "tensorboard = TensorBoard(log_dir = tb_logdir_cur)\n",
    "\n",
    "# + add to callbacks\n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then open Tensorboard in browser:\n",
    "\n",
    "http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Training options\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(metrics)\n",
    "print(\"Batch size:\", batch_size, \"Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE MODEL\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_epochs is only for the case that we execute the next code box multiple times (so that Tensorboard is displaying properly)\n",
    "past_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "\n",
    "history = model.fit(train_set, train_classes, \n",
    "                     validation_split=validation_split,\n",
    "                     #validation_data=(X_test,y_test), \n",
    "                     epochs=epochs, \n",
    "                     initial_epoch=past_epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     callbacks=callbacks\n",
    "                     )\n",
    "\n",
    "past_epochs += epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute probabilities for the classes (= get outputs of output layer)\n",
    "test_pred_prob = model.predict(test_set)\n",
    "test_pred_prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the predicted class we have to round 0 < 0.5 > 1\n",
    "test_pred = np.round(test_pred_prob)\n",
    "test_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final Accuracy\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Genre Classification\n",
    "\n",
    "this is a single-label / multi-class task (multiple categories, but decision needs to be for 1 of them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Metadata\n",
    "\n",
    "we start with the original metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which columns are available\n",
    "tags_all = metadata.columns.tolist()\n",
    "print(tags_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['classical', 'rock', 'pop', 'jazz', 'techno'] # 'electronic', ## too little data: , 'reggae', 'metal', 'hip hop']\n",
    "\n",
    "n_genres = len(genres)\n",
    "n_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[genres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[genres].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[genres].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the single-label genre task, we only retain tracks that have EXACTLY 1 genre assigned in groundtruth\n",
    "idx = metadata[genres].sum(axis=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_metadata = metadata.loc[idx,genres]\n",
    "genre_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_metadata.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes needs to be a \"1-hot encoded\" numpy array (which our groundtruth already is! we just convert pandas to numpy)\n",
    "classes = genre_metadata.values\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = genre_metadata.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Spectrograms\n",
    "\n",
    "based on the new filelist needed for the genre task \n",
    "\n",
    "we keep n_mel_bands and frames the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we saved the audio spectrograms before, we try to load them\n",
    "load_features = True\n",
    "\n",
    "# if not, we store audio features for faster reload the next time\n",
    "save_features = True\n",
    "\n",
    "FEAT_FILE = os.path.join(DATA_PATH, \"spectrograms_genres.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_features:\n",
    "    if os.path.exists(FEAT_FILE):\n",
    "        with np.load(FEAT_FILE) as npz:\n",
    "            data = npz['data']\n",
    "            filelist = npz['filenames']\n",
    "            classes = npz['classes']\n",
    "        print(\"Loaded features successfully: \" + str(len(filelist)), \"files, dimensions:\", data.shape)\n",
    "    else:\n",
    "        load_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_features:\n",
    "    data = create_spectrograms(filelist, n_mel_bands, frames)\n",
    "\n",
    "    if save_features:\n",
    "        np.savez(FEAT_FILE, data=data, filenames=filelist, classes=classes)\n",
    "        print(\"Features stored to \" + FEAT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data (see above)\n",
    "data = standardize(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add color channel (see above)\n",
    "data = add_channel(data, n_channels=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape: we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of files)\n",
    "input_shape = data.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Set Split\n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Split retains the class balance in both sets\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes[train_index]\n",
    "    test_classes = classes[test_index]\n",
    "# Note: this for loop is only executed once if n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Parameters\n",
    "\n",
    "we use the same model as for Instrumental vs. Vocal above\n",
    "\n",
    "with a few changes in the Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #1: Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss for a single label classification task is CATEGORICAL crossentropy\n",
    "loss = 'categorical_crossentropy' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #2: Output units and activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many output units\n",
    "# IN A SINGLE LABEL MULTI-CLASS TASK with N classes, we need N output units\n",
    "output_shape = n_genres\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A SINGLE LABEL MULTI-CLASS TASK with N classes we use softmax activation to BALANCE best between the classes \n",
    "# and find the best decision for ONE class\n",
    "output_activation = 'softmax'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"genres\"\n",
    "\n",
    "tb_logdir_cur = os.path.join(TB_LOGDIR, experiment_name)\n",
    "\n",
    "# initialize TensorBoard in Python\n",
    "tensorboard = TensorBoard(log_dir = tb_logdir_cur)\n",
    "\n",
    "# + add to callbacks\n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest of Parameters\n",
    "\n",
    "stay essentially the same (or similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = adam\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "epochs = 30\n",
    "\n",
    "validation_split=0.1 \n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Training options\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(metrics)\n",
    "print(\"Batch size:\", batch_size, \"Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CompactCNN(input_shape, nb_conv = nb_conv_layers, nb_filters= nb_filters, n_mels = 96, \n",
    "                           normalize=normalization, \n",
    "                           nb_hidden = nb_hidden, dense_units = dense_units, \n",
    "                           output_shape = output_shape, activation = output_activation, \n",
    "                           dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE MODEL\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_epochs is only for the case that we execute the next code box multiple times (so that Tensorboard is displaying properly)\n",
    "past_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "\n",
    "history = model.fit(train_set, train_classes, \n",
    "                     validation_split=validation_split,\n",
    "                     #validation_data=(X_test,y_test), \n",
    "                     epochs=epochs, \n",
    "                     initial_epoch=past_epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     callbacks=callbacks\n",
    "                     )\n",
    "\n",
    "past_epochs += epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute probabilities for the classes (= get outputs of output layer)\n",
    "test_pred_prob = model.predict(test_set)\n",
    "test_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the predicted class, we take the ARG MAX of the row vectors \n",
    "test_pred = np.argmax(test_pred_prob, axis=1)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for groundtruth\n",
    "test_gt = np.argmax(test_classes, axis=1)\n",
    "test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final Accuracy\n",
    "accuracy_score(test_gt, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Mood Recognition\n",
    "\n",
    "this is a multi-label classification task (multiple categories to detect, any of them can be 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = metadata.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', len(a))\n",
    "print(a)\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select 5 moods from the original list of tags \n",
    "moods = ['funky', 'quiet', 'mellow','calm', 'sad'] ## too little data: 'happy','scary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and check the data on it\n",
    "#metadata[moods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[moods].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the single-label genre task, we only retain tracks that have AT LEAST 1 of these moods assigned in groundtruth\n",
    "idx = metadata[moods].sum(axis=1) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_metadata = metadata.loc[idx,moods]\n",
    "mood_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "mood_metadata.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes needs to be a MULTI-HOT encoded\" numpy array \n",
    "# (which our groundtruth already is! we just convert pandas to numpy)\n",
    "classes = mood_metadata.values\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = mood_metadata.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Spectrograms\n",
    "\n",
    "based on the new filelist needed for the mood task \n",
    "\n",
    "we keep n_mel_bands and frames the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we saved the audio spectrograms before, we try to load them\n",
    "load_features = True\n",
    "\n",
    "# if not, we store audio features for faster reload the next time\n",
    "save_features = True\n",
    "\n",
    "FEAT_FILE = os.path.join(DATA_PATH, \"spectrograms_moods.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_features:\n",
    "    if os.path.exists(FEAT_FILE):\n",
    "        with np.load(FEAT_FILE) as npz:\n",
    "            data = npz['data']\n",
    "            filelist = npz['filenames']\n",
    "            classes = npz['classes']\n",
    "        print(\"Loaded features successfully: \" + str(len(filelist)), \"files, dimensions:\", data.shape)\n",
    "    else:\n",
    "        load_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_features:\n",
    "    data = create_spectrograms(filelist, n_mel_bands, frames)\n",
    "\n",
    "    if save_features:\n",
    "        np.savez(FEAT_FILE, data=data, filenames=filelist, classes=classes)\n",
    "        print(\"Features stored to \" + FEAT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data (see above)\n",
    "data = standardize(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add color channel (see above)\n",
    "data = add_channel(data, n_channels=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape: we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of files)\n",
    "input_shape = data.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Set Split\n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change: We cannot use Stratified Split here as it does not make sense for a MULTI-LABEL TASK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ShuffleSplit INSTEAD OF StratifiedShuffleSplit \n",
    "\n",
    "splitter = ShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes[train_index]\n",
    "    test_classes = classes[test_index]\n",
    "# Note: this for loop is only executed once if n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Parameters\n",
    "\n",
    "we use the same model as for Instrumental vs. Vocal and Genres above\n",
    "\n",
    "with a few changes in the Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #1: Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss for a MULTI label classification task is BINARY crossentropy\n",
    "loss = 'binary_crossentropy' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #2: Output units and activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many output units\n",
    "# IN A SINGLE-LABEL MULTI-CLASS or MULTI-LABEL TASK with N classes, we need N output units\n",
    "\n",
    "output_shape = n_genres\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A MULTI-LABEL TASK with N classes we use SIGMOID activation same as with a BINARY task\n",
    "# as EACH of the classes can be 0 or 1 \n",
    "\n",
    "output_activation = 'sigmoid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"moods\"\n",
    "\n",
    "tb_logdir_cur = os.path.join(TB_LOGDIR, experiment_name)\n",
    "\n",
    "# initialize TensorBoard in Python\n",
    "tensorboard = TensorBoard(log_dir = tb_logdir_cur)\n",
    "\n",
    "# + add to callbacks\n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest of Parameters\n",
    "\n",
    "stay essentially the same (or similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = adam\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "epochs = 30\n",
    "\n",
    "validation_split=0.1 \n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Training options\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(metrics)\n",
    "print(\"Batch size:\", batch_size, \"Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE MODEL\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_epochs is only for the case that we execute the next code box multiple times (so that Tensorboard is displaying properly)\n",
    "past_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "\n",
    "history = model.fit(train_set, train_classes, \n",
    "                     validation_split=validation_split,\n",
    "                     #validation_data=(X_test,y_test), \n",
    "                     epochs=epochs, \n",
    "                     initial_epoch=past_epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     callbacks=callbacks\n",
    "                     )\n",
    "\n",
    "past_epochs += epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute probabilities for the classes (= get outputs of output layer)\n",
    "test_pred_prob = model.predict(test_set)\n",
    "test_pred_prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the predicted class we have to round 0 < 0.5 > 1\n",
    "test_pred = np.round(test_pred_prob)\n",
    "test_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final Accuracy\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
